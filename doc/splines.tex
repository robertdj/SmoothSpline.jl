\documentclass[a4paper]{scrartcl}

% Text
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{csquotes}

% Math
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{bm}
	\newcommand{\vv}[1]{\ensuremath{\bm{#1}}}
	\newcommand{\mat}[1]{\ensuremath{\bm{#1}}}

    \newcommand{\T}[1]{\ensuremath{{#1}^{T}}}

	\newcommand\dd{\ensuremath{\:d}}

	\newcommand\R{\ensuremath{\mathbb{R}}}
	\newcommand\Z{\ensuremath{\mathbb{Z}}}
	\DeclareMathOperator\supp{supp}
	\DeclareMathOperator\diag{diag}
	\DeclareMathOperator\tr{tr}

% Set
% \given is part of the syntax, make sure it exists
% \providecommand\given{}
\newcommand\given{\:\vert\:}
\newcommand\SetSymbol[1][]{
	\nonscript\: #1 \vert \nonscript\:
	\mathopen{} % make - etc behave as sign
	\allowbreak % in text, allow line break after the \given
}
\DeclarePairedDelimiterX\Set[1]{\{}{\}}{
	% Change is local to \Set
	\renewcommand\given{\SetSymbol[\delimsize]}
	#1
}

\usepackage{graphicx}
\usepackage{asymptote}

\usepackage[colorlinks=true]{hyperref}

\usepackage{cleveref}

\usepackage{biblatex}
\bibliography{literature.bib}

\title{Smoothing Splines}
\author{Robert Dahl Jacobsen}

\begin{document}

\maketitle

Here I document the math used in the Julia package \textit{SmoothSpline} that performs regression with B-splines with a Tikhonov regularisation.

The \textit{SmoothSpline} package is a port of the \textit{smooth.spline} function from R.
During the implementation I discovered two \enquote{bugs} in \textit{smooth.spline}:

\begin{itemize}
    \item In one computation $\tfrac13$ is hard-coded to be $0.333$. This approximation was fine when the word size of the computer was 16 bit, but not with 64 bit.
    \item Two matrix traces are computed. In the code the first two and last two entries on the diagonal are \emph{not} included in the trace. This is not mentioned in \textit{smooth.spline}'s documentation.
\end{itemize}

I will comment more on these points later in this document.


\section{B-splines}
\label{sec:bsplines}

My source for B-splines is the excellent book \cite{Piegl:Tiller:1997} with very detailed algorithms.
The only downside for a Julia implementation is that all algorithms use 0-based arrays.
I do not include further details about about evaluation of B-splines here, but note that we have algorithms for computing the values of B-splines and all of their derivatives.


In \textit{SmoothSpline} we use cubic B-splines, so the general degree $p$ from \cite{Piegl:Tiller:1997} is always $p = 3$.
We therefore use a more compact notation here:
The $i$'th spline of degree $p$ is in \cite{Piegl:Tiller:1997} denoted $N_{i,p}$, but is here simply denoted $N_i$.
However, to avoid \enquote{magic} occurences of 3, I still use $p$ in the following to denote the degree.

A collection of B-splines are determined solely by their knots.
Boundaries are handled by reusing the boundary knots:
If we have $m$ distinct breakpoints $u_1, \ldots, u_m$ we construct the B-splines from the knots, where we have augmented by $p$ endpoints in each end ($u_1$ and $u_m$ repeated $p$ times each).

The function \textit{bs} from the \{splines\} package in R can be used to compute B-splines.


\section{Regression with B-splines}
\label{sec:regression}

We consider observations $(x_i, y_i)$ for $i = 1, \ldots, n$. 
It is allowed to have multiple observations with similar $x$ values (and $y$ values).
Observation $i$ has an associated weight $w_i$, that by default is $1$.
Observations with similar $x$ values are grouped by summing their weights and taking the average of thier $y$ values.
In the following we assume that all $x$ values are distinct and that the observations are sorted by their $x$ values.

We want to perform regression with $m$ B-splines on the interval $[x_1, x_n]$.
That is, compute an approximation with the function
\begin{align*}
    f(x) = \sum_{j=1}^M \beta_j N_j(x).
\end{align*}

The design matrix of the regression problem is the matrix $\mat X$ of size $n\times m$ with entries
\begin{equation*}
    X_{i,j} = N_j(u_i).
\end{equation*}

The weights are collected in a diagonal matrix $\mat W = \diag(w_i, i = 1, \ldots, n)$.
To enforce a smoother interpolant we choose the $\beta$'s with a least squares criterion and limit the curvature (measured by the second derivative):
\begin{equation*}
    \min_{\vv \beta} \|\vv y - \mat X \sqrt{\mat W} \vv \beta\|^2 + \lambda \int_{x_1}^{x_n} \bigl\{f^{(2)}(t)\bigr\}^2 \dd t.
\end{equation*}

We can also write this as a Tikhonov regularisation
\begin{equation*}
    \min_{\vv \beta} \|\vv y - \mat X \sqrt{\mat W} \vv \beta\| + \lambda \T{\vv\beta} \mat\Sigma \vv\beta,
\end{equation*}

where the regularisation term $\mat\Sigma$ is the Gram matrix of size $m\times m$ with entries
\begin{equation*}
    \Sigma_{i,j} =
    \int_{x_1}^{x_n} N_i^{(2)}(t) N_j^{(2)}(t) \dd t.
\end{equation*}

For each value of $\lambda > 0$, we have a solution $\vv\beta$ to the corresponding regression problem, namely the solution to the equation
\begin{equation}
    \label{eq:normal_equation}
    \mat A \vv \beta
    = \bigl(\T{\mat X} \mat W \mat X + \lambda \mat\Sigma\bigr) \vv\beta
    = \T{\mat X} \mat W \vv y 
    = \widetilde{\vv y}.
\end{equation}

This can be solved fast with a few tricks.
Both $\mat X$ and $\mat\Sigma$ are \textit{banded} with lower and upper band $p$.
That is, $\mat X_{i,j} = \mat\Sigma_{i,j} = 0$ when $|i - j| > p$.
This implies that the Cholesky factorization of $\mat A$ is also banded with the same band.
More specifically, we let $\mat C$ denote the upper triangular matrix of the Cholesky factorization such that 
\begin{equation*}
    \T{\mat C} \mat C = \T{\mat X} \mat W \mat X + \lambda \mat\Sigma.
\end{equation*}

With this convention $\T{\mat C}$ is a lower triangular matrix and we can the use the classic forward and back substitution to solve \cref{eq:normal_equation}:
First solve $\T{\mat C} \vv z = \widetilde{\vv y}$ and then $\mat C \vv \beta = \vv z$.

Note that the matrix $\mat A$ may be ill-conditioned even with the Tikhonov term.
In fact, I have seen substantial differences between $\vv\beta$'s computed with generic solvers and the special solvers used in \textit{SmoothSpline}.


\subsection{Computing matrices}

Before solving \cref{eq:normal_equation} we must compute the matrices involved.

In the \textit{documentation} for \textit{smooth.spline} (and therefore not under the same license as the \textit{code}) it is noted that the Lagrange multiplier $\lambda$ is data dependent.
That is, an appropriate value for $\lambda$ depends on the values of the observations.
This makes it more delicate to choose $\lambda$.

But \textit{smooth.spline} aids the user by offering a data independent parameter \enquote{spar}, where $0 < \text{spar} \leq 1$, that is mapped to an appropriate $\lambda$.
The map is
\begin{equation*}
    \lambda = r\cdot 256^{3\cdot\text{spar} - 1},
    \quad
    r = \frac{\tr(\T{\mat X} \mat W \mat X)}{\tr(\mat\Sigma)}.
\end{equation*}

In the code, however, it is not the standard traces that are computed.
The first two entries and the last two entries of the diagonals are discarded in the sum.
I have not found documentation to explain this, but I \emph{suspect} that it is to reduce the impact of the endpoint knots that occur in multiple splines -- cf. \cref{sec:bsplines}.

Furthermore, \textit{smooth.spline} rescales the $x$ values to the closed unit interval.
This has an impact on $\mat\Sigma$, but the impact on $\mat A$ is de facto alleviated by the above mapping.


\subsubsection{Design matrix}

The design matrix to compute is straightforward once we know how to compute the splines.


\subsubsection{Gram matrix}

We have closed-form expressions for the Tikhonov matrix $\mat\Sigma$.
All spline functions are supported on (a subset of) $[u_0, u_m]$ and we first note that
\begin{equation}
    \label{eq:sigma_entry}
    \Sigma_{i,j}
    = \int_{u_0}^{u_m} N_i^{(2)}(t) N_j^{(2)}(t) \dd t
    = \sum_{k = 0}^{m - 1} \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t.
\end{equation}

With partial integration each of the integrals can be computed as

\begin{equation*}
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t
    = \Bigl[ N_i'(t) N_j^{(2)}(t) \Bigr]_{u_k}^{u_{k+1}} -
    \int_{u_k}^{u_{k+1}} N_i'(t) N_j^{(3)}(t) \dd t.
\end{equation*}

On each interval $[u_k, u_{k+1}]$, every spline is a polynomial of degree at most three.
Hence the third derivative is a constant and

\begin{equation*}
    \int_{u_k}^{u_{k+1}} N_i'(t) N_j^{(3)}(t) \dd t
    = N_j^{(3)}(u_k) \bigl(N_i(u_{k+1}) - N_i(u_k)\bigr).
\end{equation*}

Substituting all this into \cref{eq:sigma_entry} we note a telescoping sum and arrive at an expression:
\begin{equation*}
    \Sigma_{i,j}
    = N_i'(u_m) N_j^{(2)}(u_m) - N_i'(u_0) N_j^{(2)}(u_0) 
    - \sum_{k = 0}^{m - 1} N_j^{(3)}(u_k) \bigl(N_i(u_{k+1}) - N_i(u_k)\bigr).
\end{equation*}

In \textit{smooth.spline} the integral in \cref{eq:sigma_entry} is computed differently:
On each interval $[u_k, u_{k+1}]$, the function $N_i^{(2)}$ is a polynomial of degree at most one.
In the code, this polynomial is parameterized as

\begin{equation*}
    N_i^{(2)}(t) 
    = a_{i,k} + b_{i,k} (t - u_k), 
    \quad u_k\leq t\leq u_{k+1}.
\end{equation*}

Expanding parentheses in $N_i^{(2)} N_j^{(2)}$ we see that

\begin{equation*}
    N_i^{(2)}(t) N_j^{(2)}(t)
    = a_{i,k} a_{j,k} + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) (t - u_k) + b_{i,k} b_{j,k} (t - u_k)^2.
\end{equation*}

With this expression and $\Delta_k = u_{k+1} - u_k$,

\begin{align}
    \MoveEqLeft
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t 
    \nonumber
    \\
    & = \int_0^{\Delta_k} a_{i,k} a_{j,k} + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) s + b_{i,k} b_{j,k} s^2 \dd s 
    \nonumber
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \bigl(a_{i,k} b_{j,k} + a_{j,k} b_{i,k}\bigr) \frac12 \Delta_k^2 + b_{i,k} b_{j,k} \frac13 \Delta_k^3.
    \label{eq:smooth.spline_integral}
\end{align}

The slope $b_{i,k} = N_i^{(3)}(t)$, but \textit{smooth.spline} only use the second order derivatives.
We see readily that $a_{i,k} = N_i^{(2)}(u_k)$.
Let $\Delta y_{i,k} = N_i^{(2)}(u_{k+1}) - N_i^{(2)}(u_k)$.
Then $b_{i,k} = \Delta y_{i,k} / \Delta_k$.
With these expressions, many of the $\Delta_k$'s can be removed from \cref{eq:smooth.spline_integral}:

\begin{align*}
    \MoveEqLeft
    \int_{u_k}^{u_{k+1}} N_i^{(2)}(t) N_j^{(2)}(t) \dd t 
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \Bigl(a_{i,k} \frac{\Delta y_{j,k}}{\Delta_k} + a_{j,k} \frac{\Delta y_{i,k}}{\Delta_k}\Bigr) \frac12 \Delta_k^2 + \frac{\Delta y_{i,k}}{\Delta_k} \frac{\Delta y_{j,k}}{\Delta_k} \frac13 \Delta_k^3
    \\
    & = a_{i,k} a_{j,k} \Delta_k + \bigl(a_{i,k} \Delta y_{j,k} + a_{j,k} \Delta y_{i,k}\bigr) \frac12 \Delta_k + \Delta y_{i,k} \Delta y_{j,k} \frac13 \Delta_k.
\end{align*}

In \textit{smooth.spline} the value of $\tfrac13$ is hard-coded to be $0.333$.
Even in small examples this can result in entry-wise deviations of about $1\%$.


\paragraph{Speed-ups}

From the definition we readily see that $\mat\Sigma$ is symmetric.
We know from \cite[P2.1]{Piegl:Tiller:1997} that the support of $N_i$ is $[u_i, u_{i+p+1}]$ and therefore 
\begin{equation*}
    \supp N_i \cap \supp N_j = \emptyset
    \quad\text{if}\:
    |i - j| > p.
\end{equation*}
This implies that $\Sigma_{i,j} = 0$ if $|i - j| > p$ and that the sum in \cref{eq:sigma_entry} goes from $\min\{i, j\}$ to $\min\{\max\{i, j\} + p + 1, m\}$.


\subsection{Banded matrices}

A banded matrix can be represented and stored efficiently \cite[Section 1.2.5]{Golub:van_Loan:2013}.
Furthermore, there are efficient algorithms for computing the Cholesky factorization of a banded matrix \cite[Section 4.3]{Golub:van_Loan:2013}.

In R, \textit{smooth.spline} rely on the Fortran routines \textit{dpbfa} and \textit{dpbsl} from Linpack \cite{Dongarra:Moler:Bunch:Stewart:1979} to compute the Cholesky factorization and solving \cref{eq:normal_equation}, respectively.


\printbibliography

\end{document}

